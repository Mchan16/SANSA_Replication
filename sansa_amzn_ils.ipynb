{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.sansa import SANSA\n",
    "\n",
    "sansa_config = {\n",
    "       \"l2\": 20.0,\n",
    "        \"target_density\": 5e-5,\n",
    "        \"ainv_params\": {\n",
    "            \"umr_scans\": 3,\n",
    "            \"umr_finetune_steps\": 10,\n",
    "            \"umr_loss_threshold\": 1e-4,\n",
    "        },\n",
    "        \"ldlt_method\": \"icf\"\n",
    "}\n",
    "     \n",
    "sansa = SANSA.from_config(sansa_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 19:56:38,443 : [1/3] DATASET : Loading processed dataset datasets/data/amazonbook/dataset.parquet.\n"
     ]
    }
   ],
   "source": [
    "# Load amazon books data\n",
    "\n",
    "from datasets.amazonbook import Amazonbook\n",
    "\n",
    "amazonbooks_data_config = {\n",
    "    \"name\": \"amazonbook\",\n",
    "    \"rewrite\": False,\n",
    "}\n",
    "\n",
    "amazonbooks_data = Amazonbook.from_config(amazonbooks_data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 19:57:35,351 : [1/3] DATASET : Dataframe lengths | train_df: 2380730, val_df: 2380730, test_df: 2984108\n",
      "2024-04-27 19:58:22,998 : [1/3] DATASET : Removing users [50736, 52234, 41589, 13647] from test inputs.\n",
      "2024-04-27 19:58:23,631 : [1/3] DATASET : Splits information:\n",
      "2024-04-27 19:58:23,632 : [1/3] DATASET : Train split info | n_users = 52643, n_items = 91599, n_ratings = 2380730, sparsity = 99.95%\n",
      "2024-04-27 19:58:23,633 : [1/3] DATASET : Validation split info | n_users = 52643, n_items = 91599, n_ratings = 2380730, sparsity = 99.95%\n",
      "2024-04-27 19:58:23,634 : [1/3] DATASET : Test split info | n_users = 52639, n_items = 91599, n_ratings = 2380661, sparsity = 99.95%\n",
      "2024-04-27 19:58:23,635 : [1/3] DATASET : Execution of create_splits took at 99.545 seconds.\n"
     ]
    }
   ],
   "source": [
    "amazon_split_config = {\n",
    "    \"seed\": 42,\n",
    "    \"val_target_proportion\": 0.0,\n",
    "}\n",
    "\n",
    "(amazon_train, amazon_val, amazon_test), amazon_split_time = amazonbooks_data.create_splits(amazon_split_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 19:59:32,901 : [2/3] TRAINING : Train user-item matrix info | n_users = 52643, n_items = 91599, n_ratings = 2380730, sparsity = 99.95%\n",
      "2024-04-27 19:59:32,903 : [2/3] TRAINING : Item-item matrix info | shape = (91599,91599)\n",
      "2024-04-27 19:59:32,904 : [2/3] TRAINING : Training SANSA with L2=20.0, target density=0.005000%, LDL^T method=icf, approx. inverse method=umr...\n",
      "2024-04-27 19:59:32,906 : [2/3] TRAINING : Loading item-user matrix...\n",
      "2024-04-27 19:59:33,052 : [2/3] TRAINING : Constructing weights:\n",
      "2024-04-27 19:59:42,984 : [2/3] TRAINING : Constructing A...\n",
      "2024-04-27 19:59:45,343 : [2/3] TRAINING : A info | nnz: 330335853, size: 3964.4 MB\n",
      "2024-04-27 19:59:59,701 : [2/3] TRAINING : Computing incomplete LL^T decomposition...\n",
      "2024-04-27 20:00:30,668 : [2/3] TRAINING : L info | nnz: 419506, size: 5.400 MB, density: 0.005000%\n",
      "2024-04-27 20:00:30,670 : [2/3] TRAINING : Scaling columns and creating D (LL^T -> L'DL'^T)\n",
      "2024-04-27 20:00:30,684 : [2/3] TRAINING : Execution of ldlt took at 57.630 seconds.\n",
      "2024-04-27 20:00:30,685 : [2/3] TRAINING : nnz of L: 419506, size: 5.400 MB\n",
      "2024-04-27 20:00:30,686 : [2/3] TRAINING : Computing approximate inverse of L:\n",
      "2024-04-27 20:00:30,687 : [2/3] TRAINING : Calculating initial guess using 1 step of Schultz method...\n",
      "2024-04-27 20:00:30,694 : [2/3] TRAINING : Calculating approximate inverse using Uniform Minimal Residual algorithm...\n",
      "2024-04-27 20:00:30,722 : [2/3] TRAINING : Current maximum residual: 0.00130818, relative Frobenius norm squared: 0.00000001\n",
      "2024-04-27 20:00:30,723 : [2/3] TRAINING : Reached stopping criterion.\n",
      "2024-04-27 20:00:30,753 : [2/3] TRAINING : Current maximum residual: 0.00130818, relative Frobenius norm squared: 0.00000001\n",
      "2024-04-27 20:00:30,762 : [2/3] TRAINING : Execution of ainv_L took at 0.074 seconds.\n",
      "2024-04-27 20:00:30,762 : [2/3] TRAINING : nnz of L_inv: 419506, size: 5.400 MB\n",
      "2024-04-27 20:00:30,763 : [2/3] TRAINING : Constructing W = L_inv @ P...\n",
      "2024-04-27 20:00:30,788 : [2/3] TRAINING : Extracting diagonal of W.T @ D_inv @ W...\n",
      "2024-04-27 20:00:30,799 : [2/3] TRAINING : Dividing columns of W by diagonal entries...\n",
      "2024-04-27 20:00:30,943 : [2/3] TRAINING : Execution of _construct_weights took at 57.890 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Train Sansa\n",
    "sansa.train(amazon_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 20:00:33,382 : [3/3] EVALUATION : Execution of _matmat took at 0.033 seconds.\n",
      "2024-04-27 20:00:33,714 : [3/3] EVALUATION : Execution of _matmat took at 0.331 seconds.\n",
      "2024-04-27 20:00:36,075 : [3/3] EVALUATION : Execution of _predict took at 2.726 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Evaluate \n",
    "# Get all users\n",
    "users = list(amazon_test.user_encoder.classes_)\n",
    "# Get rated items of users\n",
    "users_rated = amazon_test.get_rated_items(users)\n",
    "targets = amazon_test.get_target_items(users)\n",
    "target_ids_dict = (\n",
    "    targets.groupby(\"user_id\", group_keys=True)[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "keys = list(target_ids_dict.keys())\n",
    "users_to_arange = {user: i for i, user in enumerate(keys)}\n",
    "pd.options.mode.chained_assignment = None  # suppress irrelevant warning\n",
    "users_rated[\"user_id\"] = users_rated[\"user_id\"].map(users_to_arange)\n",
    "pd.options.mode.chained_assignment = \"warn\"\n",
    "top_maxk_ids, top_maxk_scores = sansa.recommend(users_rated, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def readImageFeatures(path):\n",
    "  f = open(path, 'rb')\n",
    "  while True:\n",
    "    asin = f.read(10)\n",
    "    if asin == '': break\n",
    "    a = array.array('f')\n",
    "    a.fromfile(f, 4096)\n",
    "    yield asin, a.tolist()\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "amazon_book_2014_data = getDF('datasets/metadata/amazonbook/meta_Books.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/xtra/chan1846/item_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56009/1313606515.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcustom_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/xtra/chan1846/item_list.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbook_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/xtra/chan1846/item_list.txt'"
     ]
    }
   ],
   "source": [
    "# Get original amazon ids so we can get metadata about book description\n",
    "book_ids = []\n",
    "custom_ids = []\n",
    "\n",
    "with open(\"datasets/metadata/amazonbook/item_list.txt\") as f:\n",
    "    book_lines = f.read().splitlines()\n",
    "\n",
    "for line in book_lines:\n",
    "    entries = line.split()\n",
    "    book_id = entries[:1]\n",
    "    custom_id = entries[1:]\n",
    "    book_ids += book_id\n",
    "    custom_ids += custom_id\n",
    "\n",
    "\n",
    "book_ids = book_ids[1:]\n",
    "\n",
    "book_ids_map = {}\n",
    "\n",
    "for i in range(len(book_ids)):\n",
    "    book_ids_map[book_ids[i]] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1321892/3375833478.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_book_2014_data_filtered['custom_id'] = amazon_book_2014_data_filtered['asin'].map(book_ids_map)\n",
      "/tmp/ipykernel_1321892/3375833478.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_book_2014_data_filtered.fillna('missing_description', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>categories</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>related</th>\n",
       "      <th>brand</th>\n",
       "      <th>custom_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001055178</td>\n",
       "      <td>{'Books': 14149327}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51ZSC6TK...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>Master Georgie</td>\n",
       "      <td>Beryl Bainbridge seems drawn to disaster. Firs...</td>\n",
       "      <td>16.95</td>\n",
       "      <td>{'also_viewed': ['0349116156', '0307947726', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>26186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>000100039X</td>\n",
       "      <td>{'Books': 587803}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/81ZKLPiv...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>The Prophet</td>\n",
       "      <td>In a distant, timeless place, a mysterious pro...</td>\n",
       "      <td>3.99</td>\n",
       "      <td>{'also_bought': ['1851686274', '0785830618', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>6379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0002005395</td>\n",
       "      <td>{'Books': 10681705}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/415HPT70...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>Deafening</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>9.39</td>\n",
       "      <td>{'also_viewed': ['B00D9TM0WK', '1602861803', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>9371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0002051850</td>\n",
       "      <td>{'Books': 1100694}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/5122XJRJ...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>For Whom the Bell Tolls</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>8.99</td>\n",
       "      <td>{'also_bought': ['0684801469', '0743297334', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0002113570</td>\n",
       "      <td>{'Books': 571745}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51iMi0zY...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>In the Shadow of Man</td>\n",
       "      <td>\"An instant animal classic.\"   --Time\"Apart fr...</td>\n",
       "      <td>12.1</td>\n",
       "      <td>{'also_bought': ['0395500818', 'B004X8W72O', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>27863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0002185385</td>\n",
       "      <td>{'Books': 1571618}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41JB1K72...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>Harvey Penick's Little Red Golf Book: Lessons ...</td>\n",
       "      <td>Before titanium drivers, before oversized head...</td>\n",
       "      <td>10.99</td>\n",
       "      <td>{'also_bought': ['0671612972', '0767903447', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>28930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>000215725X</td>\n",
       "      <td>{'Books': 767068}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51xVoZwH...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>City of Djinns: A Year in Delhi</td>\n",
       "      <td>Delhi has a richly layered past, and Dalrymple...</td>\n",
       "      <td>10.7</td>\n",
       "      <td>{'also_bought': ['0307272826', '0002555107', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>1468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0002007770</td>\n",
       "      <td>{'Books': 2296729}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/511VYFJM...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>Water For Elephants</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>8.52</td>\n",
       "      <td>{'also_bought': ['0399155341', '1573222453', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>6830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0002219417</td>\n",
       "      <td>{'Books': 4010051}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41qvc1Xm...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>The Winds of War</td>\n",
       "      <td>Herman Wouk&amp;#x2019;s acclaimed novels include ...</td>\n",
       "      <td>4.99</td>\n",
       "      <td>{'also_bought': ['0316955019', 'B0001NBNGQ', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>7738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0002226901</td>\n",
       "      <td>{'Books': 2857031}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51cqfx8f...</td>\n",
       "      <td>[[Books]]</td>\n",
       "      <td>The Partisans</td>\n",
       "      <td>'A magnificent storyteller' Sunday Mirror 'The...</td>\n",
       "      <td>6.99</td>\n",
       "      <td>{'also_bought': ['B0059I6O44', '0002215470', '...</td>\n",
       "      <td>missing_description</td>\n",
       "      <td>34840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           asin            salesRank  \\\n",
       "13   0001055178  {'Books': 14149327}   \n",
       "63   000100039X    {'Books': 587803}   \n",
       "169  0002005395  {'Books': 10681705}   \n",
       "190  0002051850   {'Books': 1100694}   \n",
       "193  0002113570    {'Books': 571745}   \n",
       "376  0002185385   {'Books': 1571618}   \n",
       "377  000215725X    {'Books': 767068}   \n",
       "426  0002007770   {'Books': 2296729}   \n",
       "447  0002219417   {'Books': 4010051}   \n",
       "480  0002226901   {'Books': 2857031}   \n",
       "\n",
       "                                                 imUrl categories  \\\n",
       "13   http://ecx.images-amazon.com/images/I/51ZSC6TK...  [[Books]]   \n",
       "63   http://ecx.images-amazon.com/images/I/81ZKLPiv...  [[Books]]   \n",
       "169  http://ecx.images-amazon.com/images/I/415HPT70...  [[Books]]   \n",
       "190  http://ecx.images-amazon.com/images/I/5122XJRJ...  [[Books]]   \n",
       "193  http://ecx.images-amazon.com/images/I/51iMi0zY...  [[Books]]   \n",
       "376  http://ecx.images-amazon.com/images/I/41JB1K72...  [[Books]]   \n",
       "377  http://ecx.images-amazon.com/images/I/51xVoZwH...  [[Books]]   \n",
       "426  http://ecx.images-amazon.com/images/I/511VYFJM...  [[Books]]   \n",
       "447  http://ecx.images-amazon.com/images/I/41qvc1Xm...  [[Books]]   \n",
       "480  http://ecx.images-amazon.com/images/I/51cqfx8f...  [[Books]]   \n",
       "\n",
       "                                                 title  \\\n",
       "13                                      Master Georgie   \n",
       "63                                         The Prophet   \n",
       "169                                          Deafening   \n",
       "190                            For Whom the Bell Tolls   \n",
       "193                               In the Shadow of Man   \n",
       "376  Harvey Penick's Little Red Golf Book: Lessons ...   \n",
       "377                    City of Djinns: A Year in Delhi   \n",
       "426                                Water For Elephants   \n",
       "447                                   The Winds of War   \n",
       "480                                      The Partisans   \n",
       "\n",
       "                                           description  price  \\\n",
       "13   Beryl Bainbridge seems drawn to disaster. Firs...  16.95   \n",
       "63   In a distant, timeless place, a mysterious pro...   3.99   \n",
       "169                                missing_description   9.39   \n",
       "190                                missing_description   8.99   \n",
       "193  \"An instant animal classic.\"   --Time\"Apart fr...   12.1   \n",
       "376  Before titanium drivers, before oversized head...  10.99   \n",
       "377  Delhi has a richly layered past, and Dalrymple...   10.7   \n",
       "426                                missing_description   8.52   \n",
       "447  Herman Wouk&#x2019;s acclaimed novels include ...   4.99   \n",
       "480  'A magnificent storyteller' Sunday Mirror 'The...   6.99   \n",
       "\n",
       "                                               related                brand  \\\n",
       "13   {'also_viewed': ['0349116156', '0307947726', '...  missing_description   \n",
       "63   {'also_bought': ['1851686274', '0785830618', '...  missing_description   \n",
       "169  {'also_viewed': ['B00D9TM0WK', '1602861803', '...  missing_description   \n",
       "190  {'also_bought': ['0684801469', '0743297334', '...  missing_description   \n",
       "193  {'also_bought': ['0395500818', 'B004X8W72O', '...  missing_description   \n",
       "376  {'also_bought': ['0671612972', '0767903447', '...  missing_description   \n",
       "377  {'also_bought': ['0307272826', '0002555107', '...  missing_description   \n",
       "426  {'also_bought': ['0399155341', '1573222453', '...  missing_description   \n",
       "447  {'also_bought': ['0316955019', 'B0001NBNGQ', '...  missing_description   \n",
       "480  {'also_bought': ['B0059I6O44', '0002215470', '...  missing_description   \n",
       "\n",
       "     custom_id  \n",
       "13       26186  \n",
       "63        6379  \n",
       "169       9371  \n",
       "190       1180  \n",
       "193      27863  \n",
       "376      28930  \n",
       "377       1468  \n",
       "426       6830  \n",
       "447       7738  \n",
       "480      34840  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data has 91598 books which are all in \n",
    "\n",
    "amazon_book_2014_data_filtered = amazon_book_2014_data[amazon_book_2014_data['asin'].isin(book_ids)]\n",
    "\n",
    "# Map the custom ids the authors use to asin\n",
    "amazon_book_2014_data_filtered['custom_id'] = amazon_book_2014_data_filtered['asin'].map(book_ids_map)\n",
    "amazon_book_2014_data_filtered.fillna('missing_description', inplace=True)\n",
    "amazon_book_2014_data_filtered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            00  000   01   02   03   04   05   09   10  100  ...  zeal  \\\n",
      "custom_id                                                    ...         \n",
      "26186      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "6379       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "9371       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "1180       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "27863      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
      "91076      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "91077      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "91360      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "85676      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "89218      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "\n",
      "           zealand  zen  zero  zest  zoe  zombie  zombies  zone  zoo  \n",
      "custom_id                                                             \n",
      "26186          0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "6379           0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "9371           0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "1180           0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "27863          0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "...            ...  ...   ...   ...  ...     ...      ...   ...  ...  \n",
      "91076          0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "91077          0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "91360          0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "85676          0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "89218          0.0  0.0   0.0   0.0  0.0     0.0      0.0   0.0  0.0  \n",
      "\n",
      "[91599 rows x 13332 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='ascii', stop_words='english', min_df=0.0005)  # Initialize a TF-IDF vectorizer\n",
    "tfidf_matrix = vectorizer.fit_transform(amazon_book_2014_data_filtered['description'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()  # Get the vocabulary \n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                        columns=feature_names, \n",
    "                        index=amazon_book_2014_data_filtered['custom_id'])\n",
    "\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.00000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "      <td>91599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.00069</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>0.011744</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.014437</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.008068</td>\n",
       "      <td>0.006103</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.01381</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.007125</td>\n",
       "      <td>0.005417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.501399</td>\n",
       "      <td>0.457315</td>\n",
       "      <td>0.929752</td>\n",
       "      <td>0.886885</td>\n",
       "      <td>0.810061</td>\n",
       "      <td>0.760539</td>\n",
       "      <td>0.683940</td>\n",
       "      <td>0.539392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.463837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328551</td>\n",
       "      <td>0.689819</td>\n",
       "      <td>0.741508</td>\n",
       "      <td>0.559063</td>\n",
       "      <td>0.383457</td>\n",
       "      <td>0.638328</td>\n",
       "      <td>0.62755</td>\n",
       "      <td>0.468269</td>\n",
       "      <td>0.476611</td>\n",
       "      <td>0.502877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 13332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 00           000            01            02            03  \\\n",
       "count  91599.000000  91599.000000  91599.000000  91599.000000  91599.000000   \n",
       "mean       0.000111      0.001431      0.000341      0.000168      0.000128   \n",
       "std        0.005307      0.013347      0.011744      0.007684      0.006505   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.501399      0.457315      0.929752      0.886885      0.810061   \n",
       "\n",
       "                 04            05            09            10           100  \\\n",
       "count  91599.000000  91599.000000  91599.000000  91599.000000  91599.000000   \n",
       "mean       0.000111      0.000115      0.000086      0.001815      0.000885   \n",
       "std        0.005387      0.005979      0.004534      0.014437      0.011237   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.760539      0.683940      0.539392      1.000000      0.463837   \n",
       "\n",
       "       ...          zeal       zealand           zen          zero  \\\n",
       "count  ...  91599.000000  91599.000000  91599.000000  91599.000000   \n",
       "mean   ...      0.000081      0.000250      0.000200      0.000234   \n",
       "std    ...      0.003077      0.007704      0.008068      0.006103   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      0.328551      0.689819      0.741508      0.559063   \n",
       "\n",
       "               zest           zoe       zombie       zombies          zone  \\\n",
       "count  91599.000000  91599.000000  91599.00000  91599.000000  91599.000000   \n",
       "mean       0.000103      0.000291      0.00069      0.000380      0.000329   \n",
       "std        0.004068      0.010150      0.01381      0.008645      0.007125   \n",
       "min        0.000000      0.000000      0.00000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.00000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.00000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.00000      0.000000      0.000000   \n",
       "max        0.383457      0.638328      0.62755      0.468269      0.476611   \n",
       "\n",
       "                zoo  \n",
       "count  91599.000000  \n",
       "mean       0.000134  \n",
       "std        0.005417  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        0.502877  \n",
       "\n",
       "[8 rows x 13332 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_maxk_ids_list = top_maxk_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7540, 138, 294, 537, 22060, 428, 420, 373, 1427, 670, 12395, 76, 429, 5595, 114, 2374, 7202, 56, 7807, 374]\n"
     ]
    }
   ],
   "source": [
    "print(top_maxk_ids_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11840303495146995\n"
     ]
    }
   ],
   "source": [
    "import recmetrics\n",
    "\n",
    "amazon_book_ils = recmetrics.intra_list_similarity(top_maxk_ids_list, tfidf_df)\n",
    "\n",
    "print(amazon_book_ils)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sansa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
