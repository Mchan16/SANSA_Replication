{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.sansa import SANSA\n",
    "\n",
    "sansa_config = {\n",
    "    \"l2\": 2.5,\n",
    "    \"target_density\": 0.0005,\n",
    "    \"ainv_params\": {\n",
    "        \"umr_scans\": 4,\n",
    "        \"umr_finetune_steps\": 10,\n",
    "        \"umr_loss_threshold\": 1e-4,\n",
    "    },\n",
    "    \"ldlt_method\": \"icf\",\n",
    "    \"ldlt_params\": {},\n",
    "}\n",
    "     \n",
    "sansa = SANSA.from_config(sansa_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 12:40:05,340 : [1/3] DATASET : Creating new dataset msd:\n",
      "2024-04-29 12:40:05,341 : [1/3] DATASET : Loading raw dataset files from datasets/data/msd/ ...\n",
      "2024-04-29 12:44:26,918 : [1/3] DATASET : Unifying dataset format...\n",
      "2024-04-29 12:44:42,576 : [1/3] DATASET : Preprocessing dataset...\n",
      "2024-04-29 12:46:23,406 : [1/3] DATASET : Saving processed dataset datasets/data/msd/dataset.parquet...\n"
     ]
    }
   ],
   "source": [
    "# Load msd data. If there is a parquet file, it will not call read_raw_data\n",
    "\n",
    "from datasets.msd import MSD\n",
    "\n",
    "msd_data_config = {\n",
    "    \"name\": \"msd\",\n",
    "    \"rewrite\": False,\n",
    "}\n",
    "\n",
    "msd_dataset = MSD.from_config(msd_data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 12:57:14,136 : [1/3] DATASET : Dataframe lengths | train_df: 28917900, val_df: 2352979, test_df: 2362570\n",
      "2024-04-29 12:58:33,809 : [1/3] DATASET : Splits information:\n",
      "2024-04-29 12:58:33,813 : [1/3] DATASET : Train split info | n_users = 491355, n_items = 41140, n_ratings = 28917900, sparsity = 99.86%\n",
      "2024-04-29 12:58:33,815 : [1/3] DATASET : Validation split info | n_users = 40000, n_items = 41140, n_ratings = 1897726, sparsity = 99.88%\n",
      "2024-04-29 12:58:33,816 : [1/3] DATASET : Test split info | n_users = 40000, n_items = 41140, n_ratings = 1905499, sparsity = 99.88%\n",
      "2024-04-29 12:58:33,817 : [1/3] DATASET : Execution of create_splits took at 98.694 seconds.\n"
     ]
    }
   ],
   "source": [
    "msd_split_config = {\n",
    "    \"n_val_users\": 40000,\n",
    "    \"n_test_users\": 40000,\n",
    "    \"seed\": 42,\n",
    "    \"target_proportion\": 0.2,\n",
    "    \"targets_newest\": False,\n",
    "}\n",
    "\n",
    "\n",
    "(msd_train, msd_val, msd_test), msd_split_time = msd_dataset.create_splits(msd_split_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test use the same songs, so we don't have to get ALL song data\n",
    "# Get unencoded item ids\n",
    "item_ids = msd_dataset.item_decoder.inverse_transform(msd_train.item_encoder.classes_)\n",
    "\n",
    "# maps song id to item id\n",
    "mapping_dict = {}\n",
    "for item_id, encoded_id in zip(item_ids, msd_train.item_encoder.classes_):\n",
    "    mapping_dict[item_id] = encoded_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of songs that could be mapped to a track: 386213\n",
      "Num of songs that could not be mapped to a track: 2380\n",
      "Num of songs that have genres: 280831\n",
      "Num of songs that do not have genres: 0\n"
     ]
    }
   ],
   "source": [
    "# Get song to track mappings\n",
    "raw_item_ids_mappings = \"datasets/metadata/msd/taste_profile_song_to_tracks.txt\"\n",
    "\n",
    "# maps song id to track id\n",
    "song_to_track = {}\n",
    "no_mapping_cnt = 0\n",
    "with open(raw_item_ids_mappings) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split()\n",
    "\n",
    "        song_id = line[0]\n",
    "        # songs can map to multiple tracks\n",
    "        track_ids = line[1:]\n",
    "\n",
    "        if len(track_ids) > 0:\n",
    "            song_to_track[song_id] = track_ids[0]\n",
    "        else:\n",
    "            song_to_track[song_id] = 'DNE'\n",
    "            no_mapping_cnt += 1\n",
    "\n",
    "print(\"Num of songs that could be mapped to a track: \" + str(len(song_to_track)))\n",
    "print(\"Num of songs that could not be mapped to a track: \" + str(no_mapping_cnt))\n",
    "\n",
    "\n",
    "# Get genres of all songs\n",
    "raw_genre_data = \"datasets/metadata/msd/msd_tagtraum_cd2.txt\"\n",
    "\n",
    "track_id_genre = {}\n",
    "no_mapping_cnt = 0\n",
    "with open(raw_genre_data) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split()\n",
    "\n",
    "        track_id = line[0]\n",
    "        # tracks can have multiple genres\n",
    "        track_genres = line[1:]\n",
    "\n",
    "        if len(track_ids) > 0:\n",
    "            track_id_genre[track_id] = track_genres[0]\n",
    "        else:\n",
    "            track_id_genre[track_id] = 'No genre'\n",
    "            no_mapping_cnt += 1\n",
    "\n",
    "print(\"Num of songs that have genres: \" + str(len(track_id_genre)))\n",
    "print(\"Num of songs that do not have genres: \" + str(no_mapping_cnt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_item_ids_to_track_ids = {}\n",
    "\n",
    "for item_id in item_ids:\n",
    "    if item_id in song_to_track:\n",
    "        if song_to_track[item_id] in track_id_genre:\n",
    "            train_item_ids_to_track_ids[mapping_dict[item_id]] = track_id_genre[song_to_track[item_id]]\n",
    "        else:\n",
    "            train_item_ids_to_track_ids[mapping_dict[item_id]] = \"none\"\n",
    "    else:\n",
    "        train_item_ids_to_track_ids[mapping_dict[item_id]] = \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  genre\n",
       "0   Pop\n",
       "1  none\n",
       "2  none\n",
       "3  none\n",
       "4  Rock"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "item_genres_df = pd.DataFrame.from_dict(train_item_ids_to_track_ids, orient='index', columns=['genre'])\n",
    "item_genres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       blues  country  electronic  folk  jazz  latin  metal  new  none  pop  \\\n",
      "0        0.0      0.0         0.0   0.0   0.0    0.0    0.0  0.0   0.0  1.0   \n",
      "1        0.0      0.0         0.0   0.0   0.0    0.0    0.0  0.0   1.0  0.0   \n",
      "2        0.0      0.0         0.0   0.0   0.0    0.0    0.0  0.0   1.0  0.0   \n",
      "3        0.0      0.0         0.0   0.0   0.0    0.0    0.0  0.0   1.0  0.0   \n",
      "4        0.0      0.0         0.0   0.0   0.0    0.0    0.0  0.0   0.0  0.0   \n",
      "...      ...      ...         ...   ...   ...    ...    ...  ...   ...  ...   \n",
      "41135    0.0      0.0         0.0   0.0   0.0    0.0    0.0  1.0   0.0  0.0   \n",
      "41136    0.0      0.0         0.0   0.0   0.0    0.0    1.0  0.0   0.0  0.0   \n",
      "41137    0.0      0.0         1.0   0.0   0.0    0.0    0.0  0.0   0.0  0.0   \n",
      "41138    0.0      0.0         0.0   0.0   0.0    0.0    1.0  0.0   0.0  0.0   \n",
      "41139    0.0      0.0         0.0   0.0   0.0    0.0    0.0  0.0   0.0  0.0   \n",
      "\n",
      "       punk  rap  reggae  rnb  rock  world  \n",
      "0       0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "1       0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "2       0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "3       0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "4       0.0  0.0     0.0  0.0   1.0    0.0  \n",
      "...     ...  ...     ...  ...   ...    ...  \n",
      "41135   0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "41136   0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "41137   0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "41138   0.0  0.0     0.0  0.0   0.0    0.0  \n",
      "41139   0.0  0.0     0.0  0.0   1.0    0.0  \n",
      "\n",
      "[41140 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='ascii')  # Initialize a TF-IDF vectorizer\n",
    "tfidf_matrix = vectorizer.fit_transform(item_genres_df['genre'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()  # Get the vocabulary \n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                        columns=feature_names, \n",
    "                        index=item_genres_df.index)\n",
    "\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 13:00:14,212 : [2/3] TRAINING : Train user-item matrix info | n_users = 491355, n_items = 41140, n_ratings = 28917900, sparsity = 99.86%\n",
      "2024-04-29 13:00:14,214 : [2/3] TRAINING : Item-item matrix info | shape = (41140,41140)\n",
      "2024-04-29 13:00:14,216 : [2/3] TRAINING : Training SANSA with L2=2.5, target density=0.050000%, LDL^T method=icf, approx. inverse method=umr...\n",
      "2024-04-29 13:00:14,217 : [2/3] TRAINING : Loading item-user matrix...\n",
      "2024-04-29 13:00:16,554 : [2/3] TRAINING : Constructing weights:\n",
      "2024-04-29 13:00:31,814 : [2/3] TRAINING : Constructing A...\n",
      "2024-04-29 13:00:42,219 : [2/3] TRAINING : A info | nnz: 716070712, size: 8593.0 MB\n",
      "2024-04-29 13:01:22,550 : [2/3] TRAINING : Computing incomplete LL^T decomposition...\n",
      "2024-04-29 13:03:50,199 : [2/3] TRAINING : L info | nnz: 846061, size: 10.317 MB, density: 0.049989%\n",
      "2024-04-29 13:03:50,200 : [2/3] TRAINING : Scaling columns and creating D (LL^T -> L'DL'^T)\n",
      "2024-04-29 13:03:50,220 : [2/3] TRAINING : Execution of ldlt took at 213.664 seconds.\n",
      "2024-04-29 13:03:50,221 : [2/3] TRAINING : nnz of L: 846061, size: 10.317 MB\n",
      "2024-04-29 13:03:50,222 : [2/3] TRAINING : Computing approximate inverse of L:\n",
      "2024-04-29 13:03:50,223 : [2/3] TRAINING : Calculating initial guess using 1 step of Schultz method...\n",
      "2024-04-29 13:03:50,231 : [2/3] TRAINING : Calculating approximate inverse using Uniform Minimal Residual algorithm...\n",
      "2024-04-29 13:03:50,315 : [2/3] TRAINING : Current maximum residual: 0.17981674, relative Frobenius norm squared: 0.00059251\n",
      "2024-04-29 13:03:50,316 : [2/3] TRAINING : Performing UMR scan 1...\n",
      "2024-04-29 13:03:50,353 : [2/3] TRAINING : Execution of _umr_scan took at 0.037 seconds.\n",
      "2024-04-29 13:03:50,436 : [2/3] TRAINING : Current maximum residual: 0.17981674, relative Frobenius norm squared: 0.00059251\n",
      "2024-04-29 13:03:50,437 : [2/3] TRAINING : Performing UMR scan 2...\n",
      "2024-04-29 13:03:50,473 : [2/3] TRAINING : Execution of _umr_scan took at 0.036 seconds.\n",
      "2024-04-29 13:03:50,556 : [2/3] TRAINING : Current maximum residual: 0.17981674, relative Frobenius norm squared: 0.00059251\n",
      "2024-04-29 13:03:50,557 : [2/3] TRAINING : Performing UMR scan 3...\n",
      "2024-04-29 13:04:17,324 : [2/3] TRAINING : Execution of _umr_scan took at 26.766 seconds.\n",
      "2024-04-29 13:04:17,393 : [2/3] TRAINING : Current maximum residual: 0.02973072, relative Frobenius norm squared: 0.00007733\n",
      "2024-04-29 13:04:17,394 : [2/3] TRAINING : Reached stopping criterion.\n",
      "2024-04-29 13:04:17,463 : [2/3] TRAINING : Current maximum residual: 0.02973072, relative Frobenius norm squared: 0.00007733\n",
      "2024-04-29 13:04:17,482 : [2/3] TRAINING : Execution of ainv_L took at 27.260 seconds.\n",
      "2024-04-29 13:04:17,483 : [2/3] TRAINING : nnz of L_inv: 846249, size: 10.320 MB\n",
      "2024-04-29 13:04:17,484 : [2/3] TRAINING : Constructing W = L_inv @ P...\n",
      "2024-04-29 13:04:17,509 : [2/3] TRAINING : Extracting diagonal of W.T @ D_inv @ W...\n",
      "2024-04-29 13:04:17,527 : [2/3] TRAINING : Dividing columns of W by diagonal entries...\n",
      "2024-04-29 13:04:17,730 : [2/3] TRAINING : Execution of _construct_weights took at 241.174 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Train Sansa\n",
    "sansa.train(msd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 13:07:22,594 : [3/3] EVALUATION : Execution of _matmat took at 0.128 seconds.\n",
      "2024-04-29 13:07:27,022 : [3/3] EVALUATION : Execution of _matmat took at 4.426 seconds.\n",
      "2024-04-29 13:07:37,990 : [3/3] EVALUATION : Execution of _predict took at 15.523 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "# Get all users\n",
    "users = list(msd_test.user_encoder.classes_)\n",
    "# Get rated items of users\n",
    "users_rated = msd_test.get_rated_items(users)\n",
    "targets = msd_test.get_target_items(users)\n",
    "target_ids_dict = (\n",
    "    targets.groupby(\"user_id\", group_keys=True)[\"item_id\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "keys = list(target_ids_dict.keys())\n",
    "users_to_arange = {user: i for i, user in enumerate(keys)}\n",
    "pd.options.mode.chained_assignment = None  # suppress irrelevant warning\n",
    "users_rated[\"user_id\"] = users_rated[\"user_id\"].map(users_to_arange)\n",
    "pd.options.mode.chained_assignment = \"warn\"\n",
    "top_maxk_ids, top_maxk_scores = sansa.recommend(users_rated, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37234763157894735\n"
     ]
    }
   ],
   "source": [
    "import recmetrics\n",
    "\n",
    "top_maxk_ids_list = top_maxk_ids.tolist()\n",
    "msd_ils = recmetrics.intra_list_similarity(top_maxk_ids_list, tfidf_df)\n",
    "\n",
    "print(msd_ils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sansa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
